# Awesome-DeepLearning-Knoweledge

#### 深度学习

* 代码写一下交叉熵损失函数：  
  https://blog.csdn.net/b1055077005/article/details/100152102  
  https://blog.csdn.net/francislucien2017/article/details/86763319

* 交叉熵损失函数原理详解    
  https://blog.csdn.net/b1055077005/article/details/100152102

* 常见的损失函数:    
  https://mp.weixin.qq.com/s/C45DxRB-n4zuxpzWrHdCzg

  https://zhuanlan.zhihu.com/p/401010037 pytorch

* sigmoid导数的大小范围：     
  https://mp.weixin.qq.com/s?__biz=MzU1OTAzMjE2OA==&mid=2247485271&idx=2&sn=5545f065a26efb928e410d8608053aff&chksm=fc1c3550cb6bbc460f0fe6dfa5d98097c54fe39975e5d7d32aeb58ccccbda5c32d80158f94d5&scene=27

* Relu激活函数相关

  https://github.com/GYee/CV_interviews_Q-A/blob/master/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/07_ReLU%E5%87%BD%E6%95%B0%E5%9C%A80%E5%A4%84%E4%B8%8D%E5%8F%AF%E5%AF%BC%EF%BC%8C%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%83%BD%E7%94%A8.md

* BN的公式及其含义作用等：      
  https://www.cnblogs.com/guoyaohua/p/8724433.html

* BN在inference的时候怎么加速  
  https://blog.csdn.net/qq_35985044/article/details/104609986  

* BN跨卡同步  
  https://blog.csdn.net/bestrivern/article/details/103754931

* BN解决过拟合和梯度消失   

  https://blog.csdn.net/qq_26598445/article/details/81950116

* layer Normalization介绍：      
  https://blog.csdn.net/liuxiao214/article/details/81037416

* 为什么Transformer要用LayerNorm？

  https://www.zhihu.com/question/487766088/answer/2608349938

  https://zhuanlan.zhihu.com/p/492803886

* BN和Dropout不能共用   
  https://blog.csdn.net/qq_42722197/article/details/125688510

* Dropout训练和测试不同     

  http://www.360doc.com/content/18/1203/22/54525756_799102767.shtml

  https://blog.csdn.net/weixin_42127358/article/details/125432885

* Dropout和dropconnect的区别  
  https://blog.51cto.com/u_11908275/6405405

* L1和L2     
  https://blog.csdn.net/devil_son1234/article/details/106572400

* CNN在图像上表现好的原因
  https://zhuanlan.zhihu.com/p/31727402

* CNN的参数量及FLOPs如何计算，CNN基础--卷积层计算、卷积层参数量、卷积层计算量
  https://www.zhihu.com/question/65305385

  https://zhuanlan.zhihu.com/p/395354063

* CNN与互相关

  https://zhuanlan.zhihu.com/p/33194385

* 1x1卷积

  https://mp.weixin.qq.com/s/DKR6gyXmL6qAtRL9IQRlvg

* 池化层（pooling）的反向传播（ReLu）        
  https://blog.csdn.net/qq_21190081/article/details/72871704

* CNN各种卷积类型

  https://github.com/GYee/CV_interviews_Q-A/blob/master/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/48_%E5%90%84%E7%A7%8D%E5%8D%B7%E7%A7%AF%E6%96%B9%E5%BC%8F%E4%B8%B2%E8%AE%B2.md

* CNN卷积及代码实现

  https://blog.csdn.net/Biyoner/article/details/88916247

* F1-score的好处

  https://www.cnblogs.com/walter-xh/p/11140715.html

* ROC和AUC    
  https://www.cnblogs.com/gatherstars/p/6084696.html

* 过拟合与欠拟合及方差偏差

  https://www.jianshu.com/p/f2489ccc14b4

* 深度学习中图像为什么要归一化？

  https://www.zhihu.com/question/293640354

* 为什么需要对数据进行归一化

  https://blog.csdn.net/qq_32172681/article/details/100876348

* 一个框架看懂优化算法之异同 SGD/AdaGrad/Adam：  
  https://zhuanlan.zhihu.com/p/32230623

* 标签平滑(Label smoothing)：    
  https://www.cnblogs.com/itmorn/p/11254448.html

* 自集成和自蒸馏：  
  https://blog.csdn.net/weixin_43145361/article/details/106630873

* Focal Loss与GHM 解决样本不平衡利器     
  https://zhuanlan.zhihu.com/p/80594704

  https://zhuanlan.zhihu.com/p/80594704 cross-entropy/focal-loss/GHM  

  https://www.cnblogs.com/ymjyqsx/p/9508664.html Focal loss与 OHEM    

* 感受野     
  https://www.cnblogs.com/shine-lee/p/12069176.html   
  https://blog.csdn.net/Kerrwy/article/details/82430530

  https://zhuanlan.zhihu.com/p/31004121

* 梯度消失和爆炸以及解决方法 
  https://github.com/GYee/CV_interviews_Q-A/blob/master/%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%A7%86%E8%A7%89/11_%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E7%88%86%E7%82%B8%E4%BB%A5%E5%8F%8A%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95.md
  https://blog.csdn.net/qq_17130909/article/details/80582226

* 模型参数初始化方法详解

  https://zhuanlan.zhihu.com/p/630487545

-----

#### 多模态

* MoCo、对比学习综述（MoCov1/v2/v3、SimCLR v1/v2、DINO等）

  https://zhuanlan.zhihu.com/p/639246058

* CLIP和改进工作串讲（LSeg、GroupViT、VLiD、 GLIPv1、 GLIPv2、CLIPasso）

  https://zhuanlan.zhihu.com/p/639247603

* 万字长文谈多模态预训练（UNITER、ViLBERT、CLIP、ALBEF、BLIP、METER）

  https://zhuanlan.zhihu.com/p/539906825 简短汇总

  https://zhuanlan.zhihu.com/p/412126626 《多模态预训练模型最新综述》

  https://blog.csdn.net/m0_38007695/article/details/107608162 《预训练 Bert 【 VilBERT，LXMERT，VisualBERT，Unicoder-VL，VL-BERT，ImageBERT 】》

  https://blog.csdn.net/m0_56533033/article/details/126071772 ViLBERT，VL-BERT，VisualBERT

  https://zhuanlan.zhihu.com/p/369733979 ViLT：最简单的多模态Transformer

* PSP多模态 -- Alibaba电商同款相似款搜索模型(WWW23录用)

  https://zhuanlan.zhihu.com/p/650496676

  https://zhuanlan.zhihu.com/p/145381066 FashionBert

* 大模型超详细解读 (目录) 

  https://zhuanlan.zhihu.com/p/625926419

  https://zhuanlan.zhihu.com/p/348593638 Vision Transformer , 通用 Vision Backbone 超详细解读 (目录)

----

#### 基础模型

* 简述CNN分类网络的演变脉络及各自的贡献与特点

  https://zhuanlan.zhihu.com/p/31727402

* SeNet   
  https://zhuanlan.zhihu.com/p/65459972

* MobileNets系列 深度可分离卷积
  https://www.cnblogs.com/dengshunge/p/11334640.html

* ShuffleNet系列    
  https://www.cnblogs.com/hellcat/p/10318630.html

* SqueezeNet系列   
  https://zhuanlan.zhihu.com/p/49465950

* EfficientNet       

  https://zhuanlan.zhihu.com/p/273234587

  https://blog.csdn.net/weixin_38346042/article/details/125813186

* GCNet and Non-local    
  https://zhuanlan.zhihu.com/p/64988633

* Nottleneck Layer     
  https://blog.csdn.net/zqnnn/article/details/88241852

* InceptionV1-V4   
  https://www.cnblogs.com/haiyang21/p/7243200.html

* DenseNet    
  https://blog.csdn.net/u014380165/article/details/75142664/

* GCN    
  https://zhuanlan.zhihu.com/p/107162772

  https://blog.csdn.net/weixin_42052081/article/details/89108966

* 深度解读：RepVGG

  https://zhuanlan.zhihu.com/p/353697121

  https://zhuanlan.zhihu.com/p/352239591

* VIT论文
  * https://zhuanlan.zhihu.com/p/340149804 transformer详解
  * https://zhuanlan.zhihu.com/p/627025622 VIT详解
  * https://zhuanlan.zhihu.com/p/535964325 ViT(Vision Transformer)的改进算法合集
  * https://zhuanlan.zhihu.com/p/367111046 图解Swin
  * https://zhuanlan.zhihu.com/p/430047908 Swin Transformer解读

----

#### 目标检测及分割

* NMS   
  https://zhuanlan.zhihu.com/p/54709759

* SmoothL1/IoU/GIoU/DIoU/CIoU Loss   
  https://zhuanlan.zhihu.com/p/104236411

* 解决小目标检测！多尺度方法汇总  
  https://zhuanlan.zhihu.com/p/141954282

  https://mp.weixin.qq.com/s/OBOEQTp0-76TlHsAucOEGQ 

* 密集物体检测   
  https://blog.csdn.net/weixin_41876817/article/details/83054525

* Yolo系列  
  https://blog.csdn.net/yuanlulu/article/details/89319839 v1-v3
  https://zhuanlan.zhihu.com/p/143747206 yolov3-v4

  https://zhuanlan.zhihu.com/p/172121380 yolov5

  https://zhuanlan.zhihu.com/p/668516241?utm_id=0 （yolov8）
  https://blog.csdn.net/qq_40716944/article/details/114822515（v1-v7）

  https://blog.csdn.net/qq_38585926/article/details/130055670 （v1-v8）

* YoloV3里面darknet53模型特点，模型里面下采样用的什么：
  https://blog.csdn.net/qq_37541097/article/details/81214953

* YOLOv2、v3使用K-means聚类计算anchor boxes的具体方法
  https://blog.csdn.net/fu18946764506/article/details/89485493

* SSD模型详解   
  https://blog.csdn.net/baidu_41848695/article/details/100023053

* SSD与Yolo的区别：   
  https://blog.csdn.net/BlowfishKing/article/details/80485006

* Faster rcnn   
  https://zhuanlan.zhihu.com/p/31426458   
  https://www.cnblogs.com/dudumiaomiao/p/6560841.html

* Faster rcnn、SSD和yolo系列的正负样本标定   
  https://blog.csdn.net/xiaotian127/article/details/104661466
  https://zhuanlan.zhihu.com/p/138824387

* 关于anchor分配相关论文：

  https://zhuanlan.zhihu.com/p/563939064
  https://mp.weixin.qq.com/s/-ir_kdOOBmotptJ0JVhunA

* 目标检测中的Anchor详解 

  https://www.cnblogs.com/wangguchangqing/p/12012508.html#autoid-1-0-0

  https://zhuanlan.zhihu.com/p/63024247

* FPN的特征在不同层怎么处理的？    
  https://blog.csdn.net/weixin_40683960/article/details/79055537

* 令人拍案称奇的Mask RCNN    
  https://zhuanlan.zhihu.com/p/37998710
  https://blog.csdn.net/wangdongwei0/article/details/83110305

* ROI pooling / ROI Align / ROI Wraping  
  https://www.cnblogs.com/wangyong/p/8523814.html  历史
  https://blog.csdn.net/thisiszdy/article/details/89058768 有反向传播
  https://zhuanlan.zhihu.com/p/73138740 有图片

* Cascade R-CNN 详细解读   
  https://zhuanlan.zhihu.com/p/42553957

* FCOS、FPN分配公式、CenterNess的作用？   

  https://blog.csdn.net/weixin_45377629/article/details/124844405

  https://zhuanlan.zhihu.com/p/63868458      

* 基于关键点的目标检测    
  https://www.cnblogs.com/yumoye/p/11029465.html （CornerNet）  
  https://zhuanlan.zhihu.com/p/66048276   （CenterNet）

  https://zhuanlan.zhihu.com/p/96856635 （Cornernet/Centernet代码里面GT heatmap）
  https://zhuanlan.zhihu.com/p/64522910 （RepPoints）

* EfficientDet  
  https://zhuanlan.zhihu.com/p/129776902

* DETR相关论文 
  https://zhuanlan.zhihu.com/p/641756060 https://zhuanlan.zhihu.com/p/144974069 （DETR）

  https://zhuanlan.zhihu.com/p/372116181   https://zhuanlan.zhihu.com/p/561191638 (deformable DETR)

  https://zhuanlan.zhihu.com/p/540786844 https://zhuanlan.zhihu.com/p/562188687（DINO）

* 目标检测指标 mAP 
  https://zhuanlan.zhihu.com/p/48992451

* 全卷积分割神经网络FCN

  https://zhuanlan.zhihu.com/p/145849717 语义分割

  https://www.coonote.com/note/fcn.html

----

#### NLP

* 完全图解RNN、RNN变体、Seq2Seq、Attention机制         
  https://zhuanlan.zhihu.com/p/28054589

* 一文搞懂RNN（循环神经网络）基础篇   
  https://zhuanlan.zhihu.com/p/30844905


----

#### 机器学习

* 决策树(Decision Tree)   
  https://zhuanlan.zhihu.com/p/30059442

* PCA   
  https://www.zhihu.com/question/41120789/answer/481966094

* 零基础学SVM    
  https://zhuanlan.zhihu.com/p/24638007

* kmeans   
  https://zhuanlan.zhihu.com/p/75477709

* LR逻辑回归   
  https://zhuanlan.zhihu.com/p/73608677

* 随机森林   
  https://www.zhihu.com/question/64043740/answer/644998828

* 集成学习（Ensemble Learning）  
  https://www.cnblogs.com/zongfa/p/9304353.html

* 八种常见机器学习对比     
  https://blog.csdn.net/Mason_Mao/article/details/82693701

* ROC曲线   AUC
  https://blog.csdn.net/qq_30992103/article/details/99730059
  https://www.bioinfo-scrounger.com/archives/767/

----

#### Pytorch

* Pytorch详解NLLLoss和CrossEntropyLoss：          
  https://blog.csdn.net/qq_22210253/article/details/85229988  
  https://github.com/mepeichun/Efficient-Neural-Network-Bilibili/blob/master/4-Knowledge-Distillation

* Pytorch中BCEloss, BCEwithlogitsloss的区别：
  https://blog.csdn.net/qq_22210253/article/details/85222093

* Pytorch .detach的作用：  
  https://blog.csdn.net/qq_39709535/article/details/80804003

* Pytorch Hook    
  https://blog.csdn.net/weixin_41555165/article/details/127454644

* pytorch中的广播机制   

```
1. A.ndim > B.ndim, 并且A.shape最后几个元素包含B.shape, 比如下面三种情况, 注意不要混淆ndim和shape这两个基本概念
    A.shape=(2,3,4,5), B.shape=(3,4,5)
    A.shape=(2,3,4,5), B.shape=(4,5)
    A.shape=(2,3,4,5), B.shape=(5)
2. A.ndim == B.ndim, 并且A.shape和B.shape对应位置的元素要么相同要么其中一个是1, 比如
    A.shape=(1,9,4), B.shape=(15,1,4)
    A.shape=(1,9,4), B.shape=(15,1,1)

https://blog.csdn.net/littlehaes/article/details/103807303
```

* Sequential的三种写法

```
net1 = nn.Sequential()
net1.add_module('conv', nn.Conv2d(3, 3, 3))
net1.add_module('batchnorm', nn.BatchNorm2d(3))
net1.add_module('activation_layer', nn.ReLU())
 
net2 = nn.Sequential(
        nn.Conv2d(3, 3, 3),
        nn.BatchNorm2d(3),
        nn.ReLU()
        )
 
from collections import OrderedDict
net3= nn.Sequential(OrderedDict([
          ('conv1', nn.Conv2d(3, 3, 3)),
          ('bn1', nn.BatchNorm2d(3)),
          ('relu1', nn.ReLU())
        ]))
```

* 调整学习率的方法

```
# 方法1: 调整学习率，新建一个optimizer
old_lr = 0.1
optimizer1 =optim.SGD([
                {'params': net.features.parameters()},
                {'params': net.classifier.parameters(), 'lr': old_lr*0.1}
            ], lr=1e-5)
 
# 方法2: 调整学习率, 手动decay, 保存动量
for param_group in optimizer.param_groups:
    param_group['lr'] *= 0.1 # 学习率为之前的0.1倍
```

* nn.functional中的函数和nn.Module主要区别：

```
1. nn.Module实现的layers是一个特殊的类，都是有class layer(nn.Module)定义，会自动提取可学习的参数
2. nn.functional中的函数更像是纯函数，由def function(input)定义
3. 也就是说如果模型有可学习的参数，最好用nn.Module否则使用哪个都可以，二者在性能上没多大差异，
4. 对于卷积，全连接等具有可学习参数的网络建议使用nn.Module
5. 激活函数（ReLU,sigmoid,tanh），池化等可以使用functional替代。对于不具有可学习参数的层，将他们用函数代替，这样可以不用放在构造函数__init__中。
```

* pytorch多卡训练的原理    
  https://blog.csdn.net/wyz6666/article/details/99484326

```
（1）将模型加载到一个指定的主GPU上，然后将模型浅拷贝到其它的从GPU上；
（2）将总的batch数据等分到不同的GPU上（坑：需要先将数据加载到主GPU上）；
（3）每个GPU根据自己分配到的数据进行forward计算得到loss，并通过backward得到权重梯度；
（4）主GPU将所有从GPU得到的梯度进行合并并用于更新模型的参数。
    https://blog.csdn.net/wyz6666/article/details/99484326
```

* pytorch中train和eval有什么不同

```
(1). model.train()——训练时候启用
启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为True
(2). model.eval()——验证和测试时候启用
不启用 BatchNormalization 和 Dropout，将BatchNormalization和Dropout置为False

train模式会计算梯度，eval模式不会计算梯度。
```

* DataLoader, DataSet, Sampler之间的关系
  https://www.cnblogs.com/marsggbo/p/11308889.html

-----

#### 编程语言（Python）

* python 深拷贝浅拷贝：   
  https://blog.csdn.net/zhubaoJay/article/details/90897028  
  https://www.csdn.net/gather_2f/MtjaIgysMjQwLWJsb2cO0O0O.html

* python 序列化与反序列化   
  https://www.cnblogs.com/wangchunli-blogs/p/9949671.html

* python垃圾回收机制   

```
python采用的是引用计数机制为主，标记-清除和分代收集(隔代回收、分代回收)两种机制为辅的策略
计数机制
Python的GC模块主要运用了引用计数来跟踪和回收垃圾。
在引用计数的基础上，还可以通过“标记-清除”解决容器对象可能产生的循环引用的问题。通过分代回收以空间换取时间进一步提高垃圾回收的效率。
标记-清除：
标记-清除的出现打破了循环引用，也就是它只关注那些可能会产生循环引用的对象
缺点：该机制所带来的额外操作和需要回收的内存块成正比。
隔代回收
原理：将系统中的所有内存块根据其存活时间划分为不同的集合，每一个集合就成为一个“代”，垃圾收集的频率随着“代”的存活时间的增大而减小。也就是说，活得越长的对象，就越不可能是垃圾，就应该减少对它的垃圾收集频率。那么如何来衡量这个存活时间：通常是利用几次垃圾收集动作来衡量，如果一个对象经过的垃圾收集次数越多，可以得出：该对象存活时间就越长。
```

* GIL全局解释器锁

```
在Cpython解释器才有GIL的概念，不是python的特点。
python在设计的时候，还没有多核的概念。因此，为了设计方便与线程安全，直接设计了一个锁：GIL锁
在一个进程下，一次只能有一个线程执行，以此来保证数据的安全性。
从这也可以看出，为多线程分配多个CPU，多个CPU也不会起作用，因为每次只能执行一个线程。所以python中的线程只能实现并发，不能实现真正的并行。
```

* is和==的区别？

```
is:判断内存地址是否相等
==：判断数值是否相等
```

* python中闭包，闭包的实质   
  https://www.jianshu.com/p/5582ca53d53e


* 解释继承

```
一个类继承自另一个类，也可以说是一个孩子类/派生类/子类，继承自父类/基类/超类，同时获取所有的类成员（属性和方法）。
继承使我们可以重用代码，并且还可以更方便地创建和维护代码。Python 支持以下类型的继承：
单继承- 一个子类类继承自单个基类
多重继承- 一个子类继承自多个基类
多级继承- 一个子类继承自一个基类，而基类继承自另一个基类
分层继承- 多个子类继承自同一个基类
混合继承- 两种或两种以上继承类型的组合
```

* 迭代器和生成器的区别

```
迭代器是一个更加抽象的概念，任何对象，如果它的类有next方法和iter方法返回自身。对于string、list、dict、tuple等这类容器对象，使用for循环遍历是很方便的。在后台for语句对容器对象调用iter()函数，iter()是Python的内置函数。iter()会返回一个定义了next()方法的迭代器对象，它在容器中逐个访问容器内元素，next()也是python的内置函数。在没有后续元素时，next()会抛出一个StopIterration的异常。
生成器（Generator）是创建迭代器的简单而强大的工具。它们写起来就像是正规的函数，只是在返回数据的时候需要使用yield语句。每次next()被调用时，生成器会返回它脱离的位置（它记忆语句最后一次执行的位置和所有的数据值）
```

* python 装饰器  
  https://www.zhihu.com/question/26930016/answer/1047233982

* python中关于unicode,utf-8,gbk等编码   
  https://blog.csdn.net/feiyang5260/article/details/81947444

* Python中*args、**args      
  https://blog.csdn.net/qq_41877039/article/details/97623476

-----

#### 编程语言（C++）

* C++三大特性之多态      
  https://blog.csdn.net/skySongkran/article/details/82012698

* C++三大特性   
  https://blog.csdn.net/qq_43414142/article/details/100892336

* C++ 静多态与动多态       
  https://www.cnblogs.com/staring-hxs/p/3669497.html

* C++继承中重载、重写、重定义、虚函数  
  https://blog.csdn.net/AndyYoung77/article/details/90146893

* C/C++ 全局变量和局部变量在内存里的区别    
  https://blog.csdn.net/luke_sanjayzzzhong/article/details/102469462

-----

#### 常见代码

* 目标检测常用代码  
  https://github.com/miaoshuyu/object-detection-usages
